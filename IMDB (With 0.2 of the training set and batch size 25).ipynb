{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from helpers.utils import creat_datasets, creating_val_data, reset_graph, grid_serach, read_pickle_file, write_pickle_file\n",
    "from helpers.utils import creating_train_val_test_datasets, tensorboard_callback,  save_best_model_callback\n",
    "from helpers.bln_layer import  bln_layer\n",
    "from helpers.dense_layer import  dense_layer\n",
    "from helpers.bln_callback import bln_callback \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 100\n",
    "minibatch = 25\n",
    "buffersize = 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs = 5\n",
    "learning_rate = 0.005\n",
    "\n",
    "#((25000  - 5000 )/ 25 ) * .2\n",
    "number_batches_train = 160 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 40 # number of batches to validate, each batch of size minibatch parameter (( 5000 )/ 25 ) * .2\n",
    "num_classes = 1\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = creating_val_data(x_train, y_train,\n",
    "                                                       number_valid_sampels = number_valid_sampels,\n",
    "                                                       random_seed=random_seed)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = creating_train_val_test_datasets(x_train, y_train,\n",
    "                                                                              x_test, y_test,\n",
    "                                                                              x_valid, y_valid, \n",
    "                                                                              minibatch = minibatch,\n",
    "                                                                              buffersize= buffersize,\n",
    "                                                                              random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((25, 80), (25,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Batch Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (80), max_features = 20000, embed_size=256, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 10,\n",
    "                          b_mm = True, b_mv = True,\n",
    "                          f_mm = False, f_mv = False):\n",
    "    \n",
    "   \n",
    "    # building the model\n",
    "  \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    \n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn2', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn3', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x)\n",
    "           \n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape= (80), max_features = max_features, \n",
    "                                        embed_size= 256, random_seed = random_seed,\n",
    "                                        lstm_unit = 128 , dense_units= num_classes,\n",
    "                                        batch_size = minibatch,\n",
    "                                        b_mm = False, b_mv = False,\n",
    "                                        f_mm = False, f_mv = False\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 256)             5120000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             197120    \n",
      "_________________________________________________________________\n",
      "bn1 (bln_layer)              (25, 80, 128)             24738     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "bn2 (bln_layer)              (25, 64)                  308       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "bn3 (bln_layer)              (25, 32)                  180       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 5,393,867\n",
      "Trainable params: 5,369,089\n",
      "Non-trainable params: 24,778\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bln_layer_imdb_batchSize_' + str(minibatch)\n",
    " \n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = [save_bm_cb, tb_cb, bln_layer_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_bln_layer.compile(optimizer = adam,\n",
    "                               loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                               metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/5\n",
      "160/160 [==============================] - 129s 808ms/step - loss: 0.6438 - binary_accuracy: 0.5725 - val_loss: 0.5718 - val_binary_accuracy: 0.6870\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 135s 842ms/step - loss: 0.3874 - binary_accuracy: 0.8215 - val_loss: 0.6508 - val_binary_accuracy: 0.7280\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 133s 832ms/step - loss: 0.1897 - binary_accuracy: 0.9230 - val_loss: 0.8240 - val_binary_accuracy: 0.7120\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 134s 838ms/step - loss: 0.0937 - binary_accuracy: 0.9657 - val_loss: 0.9585 - val_binary_accuracy: 0.7240\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 138s 861ms/step - loss: 0.0648 - binary_accuracy: 0.9755 - val_loss: 1.1418 - val_binary_accuracy: 0.7260\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs=epochs,\n",
    "                                                verbose=1, callbacks=bln_layer_cb_list,\n",
    "                                                validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bln_layer_imdb_batchSize_25/best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.5972 - binary_accuracy: 0.7449\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.5926 - binary_accuracy: 0.7450\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.5969 - binary_accuracy: 0.7450\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 298s 298ms/step - loss: 0.5924 - binary_accuracy: 0.7449\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.5305 - binary_accuracy: 0.7416\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 297s 297ms/step - loss: 0.5308 - binary_accuracy: 0.7415\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 304s 304ms/step - loss: 0.5304 - binary_accuracy: 0.7417\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.5308 - binary_accuracy: 0.7416\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.5895 - binary_accuracy: 0.7137\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.5874 - binary_accuracy: 0.7142\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.5893 - binary_accuracy: 0.7136\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 305s 305ms/step - loss: 0.5873 - binary_accuracy: 0.7141\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5872673473656177, 0.71408]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 301s 301ms/step - loss: 0.5709 - binary_accuracy: 0.6900\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5872673473656177, 0.71408], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.5708734404444694, 0.68996]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 299s 299ms/step - loss: 0.5714 - binary_accuracy: 0.6901\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5872673473656177, 0.71408], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.5708734404444694, 0.68996], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.571403653472662, 0.69008]}\n",
      "session is clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.5709 - binary_accuracy: 0.6901\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5872673473656177, 0.71408], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.5708734404444694, 0.68996], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.571403653472662, 0.69008], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.5708688415884972, 0.69012]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.5714 - binary_accuracy: 0.6900\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.5971591956093907, 0.74492], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.5925961168557405, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.596867555141449, 0.74496], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.5923926493823528, 0.74492], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.5305076062381268, 0.7416], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5308341007232666, 0.74152], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.530445402622223, 0.74172], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.5307774426937103, 0.7416], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.5894799150526524, 0.71368], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5874100328683853, 0.71416], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.5892977966964245, 0.7136], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5872673473656177, 0.71408], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.5708734404444694, 0.68996], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.571403653472662, 0.69008], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.5708688415884972, 0.69012], 'Bmm_False Bmv_False Fmm_False Fmv_False': [0.5713950329124927, 0.68996]}\n",
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True,\n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path,\n",
    "                         loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics = tf.keras.metrics.BinaryAccuracy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bmm_True Bmv_False Fmm_False Fmv_True', [0.530445402622223, 0.74172]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_True', [0.5305076062381268, 0.7416]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_False', [0.5307774426937103, 0.7416]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_False', [0.5308341007232666, 0.74152]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_True', [0.5708688415884972, 0.69012]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_True', [0.5708734404444694, 0.68996]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_False', [0.5713950329124927, 0.68996]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_False', [0.571403653472662, 0.69008]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_False', [0.5872673473656177, 0.71408]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_False', [0.5874100328683853, 0.71416]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_True', [0.5892977966964245, 0.7136]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_True', [0.5894799150526524, 0.71368]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_False', [0.5923926493823528, 0.74492]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_False', [0.5925961168557405, 0.74496]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_True', [0.596867555141449, 0.74496]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_True', [0.5971591956093907, 0.74492])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using  Batch Normalization implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape= (80), max_features = 20000, embed_size=256, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn1') (x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn2') (x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn3') (x) \n",
    "\n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 256)             5120000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             197120    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (25, 80, 128)             512       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (25, 64)                  256       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (25, 32)                  128       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 5,369,537\n",
      "Trainable params: 5,369,089\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 256, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "model_bn_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bn_Keras_imdb_batchSize_' + str(minibatch)\n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/5\n",
      "160/160 [==============================] - 133s 829ms/step - loss: 0.7066 - binary_accuracy: 0.5238 - val_loss: 0.6666 - val_binary_accuracy: 0.4880\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 132s 824ms/step - loss: 0.5074 - binary_accuracy: 0.7287 - val_loss: 0.6424 - val_binary_accuracy: 0.6360\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 130s 815ms/step - loss: 0.3155 - binary_accuracy: 0.8543 - val_loss: 0.8765 - val_binary_accuracy: 0.6090\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 132s 825ms/step - loss: 0.2473 - binary_accuracy: 0.8985 - val_loss: 1.0465 - val_binary_accuracy: 0.6300\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 131s 821ms/step - loss: 0.1971 - binary_accuracy: 0.9260 - val_loss: 0.9727 - val_binary_accuracy: 0.6810\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs = epochs, verbose=1, \n",
    "                                             callbacks = bn_keras_cb_list,\n",
    "                                             validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bn_Keras_imdb_batchSize_25/best_model.h5'\n",
    "model_bn_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 296s 296ms/step - loss: 0.6393 - binary_accuracy: 0.6297\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_test_history = model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape= (80), max_features = 20000, embed_size=256, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 256)             5120000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             197120    \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (25, 80, 128)             256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (25, 64)                  128       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (25, 32)                  64        \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 5,369,089\n",
      "Trainable params: 5,369,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 256, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_ln_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/ln_Keras_imdb_batchSize_' + str(minibatch) \n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/5\n",
      "160/160 [==============================] - 131s 820ms/step - loss: 0.7054 - binary_accuracy: 0.4970 - val_loss: 0.7082 - val_binary_accuracy: 0.4880\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 132s 826ms/step - loss: 0.6956 - binary_accuracy: 0.4985 - val_loss: 0.7078 - val_binary_accuracy: 0.4880\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 132s 825ms/step - loss: 0.6740 - binary_accuracy: 0.5332 - val_loss: 0.6716 - val_binary_accuracy: 0.6330\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 129s 805ms/step - loss: 0.4677 - binary_accuracy: 0.7632 - val_loss: 0.7473 - val_binary_accuracy: 0.6920\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 130s 810ms/step - loss: 0.2513 - binary_accuracy: 0.9000 - val_loss: 0.7424 - val_binary_accuracy: 0.7170\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "\n",
    "weights_path = './models/rnn/ln_Keras_imdb_batchSize_25/best_model.h5'\n",
    "\n",
    "model_ln_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 300s 300ms/step - loss: 0.6884 - binary_accuracy: 0.6393\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_test_history = model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.writing pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_25/model_ln_keras_history_25.pickle', model_ln_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_25/model_ln_keras_test_history_25.pickle', model_ln_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_25/model_bn_keras_history_25.pickle', model_bn_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_25/model_bn_keras_test_history_25.pickle', model_bn_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_25/model_bln_keras_history_25.pickle', model_bln_layer_history.history)\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_25/model_bln_keras_test_history_25.pickle', evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
