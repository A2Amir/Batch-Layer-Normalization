{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from helpers.utils import creat_datasets, creating_val_data, reset_graph, grid_serach, read_pickle_file, write_pickle_file\n",
    "from helpers.utils import creating_train_val_test_datasets, tensorboard_callback,  save_best_model_callback\n",
    "from helpers.bln_layer import  bln_layer\n",
    "from helpers.dense_layer import  dense_layer\n",
    "from helpers.bln_callback import bln_callback \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 100\n",
    "minibatch = 25\n",
    "buffersize = 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs = 10\n",
    "learning_rate = 0.003 #0.005\n",
    "\n",
    "#((25000  - 5000 )/ 25 ) * .2\n",
    "number_batches_train = 160 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 40 # number of batches to validate, each batch of size minibatch parameter (( 5000 )/ 25 ) * .2\n",
    "num_classes = 1\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = creating_val_data(x_train, y_train,\n",
    "                                                       number_valid_sampels = number_valid_sampels,\n",
    "                                                       random_seed=random_seed)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = creating_train_val_test_datasets(x_train, y_train,\n",
    "                                                                              x_test, y_test,\n",
    "                                                                              x_valid, y_valid, \n",
    "                                                                              minibatch = minibatch,\n",
    "                                                                              buffersize= buffersize,\n",
    "                                                                              random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((25, 80), (25,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Batch Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 10,\n",
    "                          b_mm = True, b_mv = True, f_mm = False, f_mv = False):\n",
    "    \n",
    "   \n",
    "    # building the model\n",
    "  \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    \n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn2', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn3', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x)\n",
    "           \n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape = (80), max_features = max_features, \n",
    "                                        embed_size = 512, random_seed = random_seed,\n",
    "                                        lstm_unit = 128 , dense_units= num_classes,\n",
    "                                        batch_size = minibatch,\n",
    "                                        b_mm = False, b_mv = False,\n",
    "                                        f_mm = False, f_mv = False\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 512)             10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             328192    \n",
      "_________________________________________________________________\n",
      "bn1 (bln_layer)              (25, 80, 128)             24738     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "bn2 (bln_layer)              (25, 64)                  308       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "bn3 (bln_layer)              (25, 32)                  180       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 10,644,939\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 24,778\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bln_layer_imdb_batchSize_' + str(minibatch)\n",
    " \n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = [save_bm_cb, tb_cb, bln_layer_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_bln_layer.compile(optimizer = adam,\n",
    "                               loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                               metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 141s 884ms/step - loss: 0.5864 - binary_accuracy: 0.6620 - val_loss: 0.5051 - val_binary_accuracy: 0.7690\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 143s 894ms/step - loss: 0.3282 - binary_accuracy: 0.8585 - val_loss: 0.6083 - val_binary_accuracy: 0.7660\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 146s 915ms/step - loss: 0.1776 - binary_accuracy: 0.9358 - val_loss: 0.8178 - val_binary_accuracy: 0.7220\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 145s 904ms/step - loss: 0.0986 - binary_accuracy: 0.9685 - val_loss: 0.9108 - val_binary_accuracy: 0.7460\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 146s 912ms/step - loss: 0.0523 - binary_accuracy: 0.9833 - val_loss: 1.1147 - val_binary_accuracy: 0.7450\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 145s 907ms/step - loss: 0.0424 - binary_accuracy: 0.9850 - val_loss: 1.1855 - val_binary_accuracy: 0.7530\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 145s 906ms/step - loss: 0.0329 - binary_accuracy: 0.9880 - val_loss: 1.3090 - val_binary_accuracy: 0.7370\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 146s 910ms/step - loss: 0.0182 - binary_accuracy: 0.9940 - val_loss: 1.4237 - val_binary_accuracy: 0.7400\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 144s 902ms/step - loss: 0.0147 - binary_accuracy: 0.9948 - val_loss: 1.5093 - val_binary_accuracy: 0.7430\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 145s 905ms/step - loss: 0.0097 - binary_accuracy: 0.9973 - val_loss: 1.6138 - val_binary_accuracy: 0.7430\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs=epochs,\n",
    "                                                verbose=1, callbacks=bln_layer_cb_list,\n",
    "                                                validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bln_layer_imdb_batchSize_25/best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.6630 - binary_accuracy: 0.7690\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.6498 - binary_accuracy: 0.7689\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.6629 - binary_accuracy: 0.7690\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.6497 - binary_accuracy: 0.7688\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.4988 - binary_accuracy: 0.7726\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.5000 - binary_accuracy: 0.7723\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.4987 - binary_accuracy: 0.7726\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.4999 - binary_accuracy: 0.7723\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 318s 318ms/step - loss: 0.6068 - binary_accuracy: 0.7694\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 317s 317ms/step - loss: 0.5972 - binary_accuracy: 0.7694\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 315s 315ms/step - loss: 0.6068 - binary_accuracy: 0.7694\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.5973 - binary_accuracy: 0.7694\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5972526565939188, 0.7694]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.4963 - binary_accuracy: 0.7709\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5972526565939188, 0.7694], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.4963140346705914, 0.77092]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.4963 - binary_accuracy: 0.7712\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5972526565939188, 0.7694], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.4963140346705914, 0.77092], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.49630365321040154, 0.77116]}\n",
      "session is clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 314s 314ms/step - loss: 0.4963 - binary_accuracy: 0.7708\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5972526565939188, 0.7694], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.4963140346705914, 0.77092], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.49630365321040154, 0.77116], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.4963227770626545, 0.77084]}\n",
      "session is clear\n",
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.4963 - binary_accuracy: 0.7712\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6629747842252255, 0.769], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6497624014168978, 0.76892], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6629162631332874, 0.76896], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6497238198667765, 0.76884], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.4987801894545555, 0.7726], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.4999744267165661, 0.77228], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.49874343141913413, 0.77256], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.49993953239917754, 0.77232], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6067768514379859, 0.7694], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.5972367646098137, 0.76944], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6067867462709546, 0.7694], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.5972526565939188, 0.7694], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.4963140346705914, 0.77092], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.49630365321040154, 0.77116], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.4963227770626545, 0.77084], 'Bmm_False Bmv_False Fmm_False Fmv_False': [0.4963075534701347, 0.77116]}\n",
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True,\n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path,\n",
    "                         loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics = tf.keras.metrics.BinaryAccuracy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bmm_False Bmv_False Fmm_True Fmv_False', [0.49630365321040154, 0.77116]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_False', [0.4963075534701347, 0.77116]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_True', [0.4963140346705914, 0.77092]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_True', [0.4963227770626545, 0.77084]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_True', [0.49874343141913413, 0.77256]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_True', [0.4987801894545555, 0.7726]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_False', [0.49993953239917754, 0.77232]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_False', [0.4999744267165661, 0.77228]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_False', [0.5972367646098137, 0.76944]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_False', [0.5972526565939188, 0.7694]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_True', [0.6067768514379859, 0.7694]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_True', [0.6067867462709546, 0.7694]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_False', [0.6497238198667765, 0.76884]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_False', [0.6497624014168978, 0.76892]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_True', [0.6629162631332874, 0.76896]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_True', [0.6629747842252255, 0.769])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using  Batch Normalization implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape = (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn1') (x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn2') (x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn3') (x) \n",
    "\n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 512)             10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             328192    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (25, 80, 128)             512       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (25, 64)                  256       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (25, 32)                  128       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 10,620,609\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 512, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "model_bn_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bn_Keras_imdb_batchSize_' + str(minibatch)\n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 144s 898ms/step - loss: 0.7140 - binary_accuracy: 0.5182 - val_loss: 0.6675 - val_binary_accuracy: 0.4890\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 144s 899ms/step - loss: 0.4985 - binary_accuracy: 0.7473 - val_loss: 0.6189 - val_binary_accuracy: 0.6950\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 144s 901ms/step - loss: 0.2498 - binary_accuracy: 0.9020 - val_loss: 0.7821 - val_binary_accuracy: 0.7340\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 143s 896ms/step - loss: 0.1636 - binary_accuracy: 0.9373 - val_loss: 0.9464 - val_binary_accuracy: 0.7070\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 143s 895ms/step - loss: 0.1099 - binary_accuracy: 0.9628 - val_loss: 0.9451 - val_binary_accuracy: 0.7230\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 144s 903ms/step - loss: 0.0646 - binary_accuracy: 0.9797 - val_loss: 1.2822 - val_binary_accuracy: 0.7030\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 144s 897ms/step - loss: 0.0576 - binary_accuracy: 0.9818 - val_loss: 1.4616 - val_binary_accuracy: 0.7210\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 144s 900ms/step - loss: 0.0532 - binary_accuracy: 0.9840 - val_loss: 1.1869 - val_binary_accuracy: 0.7010\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 145s 907ms/step - loss: 0.0919 - binary_accuracy: 0.9672 - val_loss: 1.1390 - val_binary_accuracy: 0.7260\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 144s 899ms/step - loss: 0.0671 - binary_accuracy: 0.9790 - val_loss: 1.4611 - val_binary_accuracy: 0.7070\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs = epochs, verbose = 1, \n",
    "                                             callbacks = bn_keras_cb_list,\n",
    "                                             validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bn_Keras_imdb_batchSize_25/best_model.h5'\n",
    "model_bn_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 316s 316ms/step - loss: 0.6203 - binary_accuracy: 0.6982\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_test_history = model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape = (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(25, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (25, 80, 512)             10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (25, 80, 128)             328192    \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (25, 80, 128)             256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (25, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (25, 64)                  128       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (25, 32)                  2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (25, 32)                  0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (25, 32)                  64        \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (25, 1)                   33        \n",
      "=================================================================\n",
      "Total params: 10,620,161\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 512, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_ln_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/ln_Keras_imdb_batchSize_' + str(minibatch) \n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 144s 898ms/step - loss: 0.7015 - binary_accuracy: 0.4997 - val_loss: 0.7034 - val_binary_accuracy: 0.4880\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 143s 893ms/step - loss: 0.5430 - binary_accuracy: 0.7085 - val_loss: 0.6060 - val_binary_accuracy: 0.7330\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 144s 898ms/step - loss: 0.2694 - binary_accuracy: 0.8888 - val_loss: 0.8571 - val_binary_accuracy: 0.6910\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 142s 890ms/step - loss: 0.1182 - binary_accuracy: 0.9580 - val_loss: 0.9089 - val_binary_accuracy: 0.7580\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 143s 894ms/step - loss: 0.0643 - binary_accuracy: 0.9790 - val_loss: 0.9540 - val_binary_accuracy: 0.7570\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 143s 894ms/step - loss: 0.0286 - binary_accuracy: 0.9925 - val_loss: 1.0657 - val_binary_accuracy: 0.7540\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 142s 890ms/step - loss: 0.0249 - binary_accuracy: 0.9910 - val_loss: 1.1088 - val_binary_accuracy: 0.7460\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 142s 890ms/step - loss: 0.0110 - binary_accuracy: 0.9975 - val_loss: 1.3627 - val_binary_accuracy: 0.7370\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 144s 900ms/step - loss: 0.0113 - binary_accuracy: 0.9962 - val_loss: 1.3765 - val_binary_accuracy: 0.7480\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 142s 890ms/step - loss: 0.0263 - binary_accuracy: 0.9925 - val_loss: 1.2105 - val_binary_accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "\n",
    "weights_path = './models/rnn/ln_Keras_imdb_batchSize_25/best_model.h5'\n",
    "\n",
    "model_ln_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 312s 312ms/step - loss: 0.6386 - binary_accuracy: 0.7105\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_test_history = model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.writing pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_25/model_ln_keras_history_25.pickle', model_ln_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_25/model_ln_keras_test_history_25.pickle', model_ln_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_25/model_bn_keras_history_25.pickle', model_bn_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_25/model_bn_keras_test_history_25.pickle', model_bn_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_25/model_bln_keras_history_25.pickle', model_bln_layer_history.history)\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_25/model_bln_keras_test_history_25.pickle', evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
