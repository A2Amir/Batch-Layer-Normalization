{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from helpers.utils import creat_datasets, creating_val_data, reset_graph, grid_serach, read_pickle_file, write_pickle_file\n",
    "from helpers.utils import creating_train_val_test_datasets, tensorboard_callback,  save_best_model_callback\n",
    "from helpers.bln_layer import  bln_layer\n",
    "from helpers.dense_layer import  dense_layer\n",
    "from helpers.bln_callback import bln_callback \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 100\n",
    "minibatch = 1\n",
    "buffersize = 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs = 10\n",
    "learning_rate = 0.003 \n",
    "\n",
    "#((25000  - 5000 )/ 1 ) * .2\n",
    "number_batches_train = 4000 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 1000 # number of batches to validate, each batch of size minibatch parameter (( 5000 )/ 1 ) * .2\n",
    "num_classes = 1\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = creating_val_data(x_train, y_train,\n",
    "                                                       number_valid_sampels = number_valid_sampels,\n",
    "                                                       random_seed=random_seed)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = creating_train_val_test_datasets(x_train, y_train,\n",
    "                                                                              x_test, y_test,\n",
    "                                                                              x_valid, y_valid, \n",
    "                                                                              minibatch = minibatch,\n",
    "                                                                              buffersize= buffersize,\n",
    "                                                                              random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((1, 80), (1,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Batch Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 10,\n",
    "                          b_mm = True, b_mv = True,\n",
    "                          f_mm = False, f_mv = False):\n",
    "    \n",
    "   \n",
    "    # building the model\n",
    "  \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    \n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn2', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn3', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x)\n",
    "           \n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape= (80), max_features = max_features, \n",
    "                                        embed_size = 512, random_seed = random_seed,\n",
    "                                        lstm_unit = 128 , dense_units= num_classes,\n",
    "                                        batch_size = minibatch,\n",
    "                                        b_mm = False, b_mv = False,\n",
    "                                        f_mm = False, f_mv = False\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(1, 80)]                 0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (1, 80, 512)              10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, 80, 128)              328192    \n",
      "_________________________________________________________________\n",
      "bn1 (bln_layer)              (1, 80, 128)              20898     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 64)                   49408     \n",
      "_________________________________________________________________\n",
      "bn2 (bln_layer)              (1, 64)                   260       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (1, 32)                   2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (1, 32)                   0         \n",
      "_________________________________________________________________\n",
      "bn3 (bln_layer)              (1, 32)                   132       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (1, 1)                    33        \n",
      "=================================================================\n",
      "Total params: 10,641,003\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 20,842\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bln_layer_imdb_batchSize_' + str(minibatch)\n",
    " \n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = [save_bm_cb, tb_cb, bln_layer_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_bln_layer.compile(optimizer = adam,\n",
    "                               loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                               metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4000 steps, validate for 1000 steps\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 3617s 904ms/step - loss: 0.6955 - binary_accuracy: 0.5023 - val_loss: 0.6956 - val_binary_accuracy: 0.4880\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 3605s 901ms/step - loss: 0.6904 - binary_accuracy: 0.4990 - val_loss: 0.6873 - val_binary_accuracy: 0.5060\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 3597s 899ms/step - loss: 0.5846 - binary_accuracy: 0.6610 - val_loss: 0.6869 - val_binary_accuracy: 0.6420\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 3599s 900ms/step - loss: 0.4421 - binary_accuracy: 0.7822 - val_loss: 0.6927 - val_binary_accuracy: 0.7020\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 3652s 913ms/step - loss: 0.3085 - binary_accuracy: 0.8742 - val_loss: 0.6475 - val_binary_accuracy: 0.7310\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 3628s 907ms/step - loss: 0.2044 - binary_accuracy: 0.9215 - val_loss: 0.6940 - val_binary_accuracy: 0.7490\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 3656s 914ms/step - loss: 0.1339 - binary_accuracy: 0.9542 - val_loss: 0.8082 - val_binary_accuracy: 0.7450\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 3641s 910ms/step - loss: 0.0897 - binary_accuracy: 0.9703 - val_loss: 0.9152 - val_binary_accuracy: 0.7430\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 3631s 908ms/step - loss: 0.0749 - binary_accuracy: 0.9793 - val_loss: 1.1036 - val_binary_accuracy: 0.7110\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 3635s 909ms/step - loss: 0.0664 - binary_accuracy: 0.9790 - val_loss: 1.0779 - val_binary_accuracy: 0.7320\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs=epochs,\n",
    "                                                verbose=1, callbacks=bln_layer_cb_list,\n",
    "                                                validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bln_layer_imdb_batchSize_1/best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 7682s 307ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7722s 309ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7737s 309ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7742s 310ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 8088s 324ms/step - loss: 0.6941 - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7779s 311ms/step - loss: 0.5834 - binary_accuracy: 0.7314\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7801s 312ms/step - loss: 0.6941 - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7753s 310ms/step - loss: 0.6562 - binary_accuracy: 0.7311\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7752s 310ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7755s 310ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7749s 310ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 8340s 334ms/step - loss: nan - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_False': [nan, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7724s 309ms/step - loss: 0.6941 - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.6941002011299133, 0.5]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7750s 310ms/step - loss: 0.5839 - binary_accuracy: 0.7314\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.6941002011299133, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.5838858440081776, 0.7314]}\n",
      "session is clear\n",
      "25000/25000 [==============================] - 7793s 312ms/step - loss: 0.6941 - binary_accuracy: 0.5000\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.6941002011299133, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.5838858440081776, 0.7314], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.6941002011299133, 0.5]}\n",
      "session is clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 7806s 312ms/step - loss: 0.6570 - binary_accuracy: 0.7311\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_True Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.5833858219459653, 0.7314], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6940792372179031, 0.5], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6561968348427117, 0.73108], 'Bmm_False Bmv_True Fmm_True Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_True Fmv_False': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_True': [nan, 0.5], 'Bmm_False Bmv_True Fmm_False Fmv_False': [nan, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.6941002011299133, 0.5], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.5838858440081776, 0.7314], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.6941002011299133, 0.5], 'Bmm_False Bmv_False Fmm_False Fmv_False': [0.6570169957915694, 0.73112]}\n",
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True,\n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path,\n",
    "                         loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics = tf.keras.metrics.BinaryAccuracy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bmm_True Bmv_True Fmm_True Fmv_True', [nan, 0.5]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_False', [nan, 0.5]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_True', [nan, 0.5]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_False', [nan, 0.5]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_False', [0.5833858219459653, 0.7314]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_False', [0.5838858440081776, 0.7314]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_False', [0.6561968348427117, 0.73108]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_False', [0.6570169957915694, 0.73112]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_True', [0.6940792372179031, 0.5]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_True', [0.6940792372179031, 0.5]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_True', [nan, 0.5]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_False', [nan, 0.5]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_True', [nan, 0.5]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_False', [nan, 0.5]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_True', [0.6941002011299133, 0.5]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_True', [0.6941002011299133, 0.5])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using  Batch Normalization implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape= (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn1') (x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn2') (x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn3') (x) \n",
    "\n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(1, 80)]                 0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (1, 80, 512)              10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, 80, 128)              328192    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (1, 80, 128)              512       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 64)                   49408     \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (1, 64)                   256       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (1, 32)                   2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (1, 32)                   0         \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (1, 32)                   128       \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (1, 1)                    33        \n",
      "=================================================================\n",
      "Total params: 10,620,609\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape = (80), max_features = max_features, \n",
    "                                embed_size = 512, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "model_bn_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/bn_Keras_imdb_batchSize_' + str(minibatch)\n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4000 steps, validate for 1000 steps\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 3635s 909ms/step - loss: 0.6938 - binary_accuracy: 0.4985 - val_loss: 1.3533 - val_binary_accuracy: 0.4940\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 3659s 915ms/step - loss: 0.6942 - binary_accuracy: 0.4985 - val_loss: 3.0271 - val_binary_accuracy: 0.4980\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 3666s 917ms/step - loss: 0.6945 - binary_accuracy: 0.4985 - val_loss: 2.9627 - val_binary_accuracy: 0.4930\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 3642s 910ms/step - loss: 0.6944 - binary_accuracy: 0.4985 - val_loss: 2.7060 - val_binary_accuracy: 0.4980\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 3672s 918ms/step - loss: 0.6943 - binary_accuracy: 0.4985 - val_loss: 2.4211 - val_binary_accuracy: 0.4840\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 3639s 910ms/step - loss: 0.6944 - binary_accuracy: 0.4985 - val_loss: 2.2575 - val_binary_accuracy: 0.4830\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 3656s 914ms/step - loss: 0.6940 - binary_accuracy: 0.4985 - val_loss: 0.9874 - val_binary_accuracy: 0.5050\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 3660s 915ms/step - loss: 0.6944 - binary_accuracy: 0.4985 - val_loss: 2.7385 - val_binary_accuracy: 0.5370\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 3678s 920ms/step - loss: 0.6943 - binary_accuracy: 0.4985 - val_loss: 1.4995 - val_binary_accuracy: 0.5030\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 3699s 925ms/step - loss: 0.6943 - binary_accuracy: 0.4985 - val_loss: 2.9047 - val_binary_accuracy: 0.4930\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs = epochs, verbose=1, \n",
    "                                             callbacks = bn_keras_cb_list,\n",
    "                                             validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "weights_path = './models/rnn/bn_Keras_imdb_batchSize_1/best_model.h5'\n",
    "model_bn_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 7799s 312ms/step - loss: 0.9793 - binary_accuracy: 0.4946\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_test_history = model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape= (80), max_features = 20000, embed_size = 512, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1, batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2, activation='tanh',\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "    x = dense_layer(units = 32, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.Activation('tanh')(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense2', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(1, 80)]                 0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (1, 80, 512)              10240000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, 80, 128)              328192    \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (1, 80, 128)              256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 64)                   49408     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (1, 64)                   128       \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (1, 32)                   2080      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (1, 32)                   0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (1, 32)                   64        \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (1, 1)                    33        \n",
      "=================================================================\n",
      "Total params: 10,620,161\n",
      "Trainable params: 10,620,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 512, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "adam = tf.keras.optimizers.Adam(learning_rate= learning_rate, beta_1=.9, beta_2=.999, epsilon=1e-08)\n",
    "\n",
    "model_ln_keras.compile(optimizer = adam,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = '/rnn/ln_Keras_imdb_batchSize_' + str(minibatch) \n",
    "\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4000 steps, validate for 1000 steps\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 3571s 893ms/step - loss: 0.7074 - binary_accuracy: 0.5040 - val_loss: 0.6910 - val_binary_accuracy: 0.4880\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 3584s 896ms/step - loss: 0.6490 - binary_accuracy: 0.5690 - val_loss: 0.6834 - val_binary_accuracy: 0.4880\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 3578s 895ms/step - loss: 0.5711 - binary_accuracy: 0.6933 - val_loss: 0.6880 - val_binary_accuracy: 0.6240\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 3559s 890ms/step - loss: 0.4699 - binary_accuracy: 0.7918 - val_loss: 0.6675 - val_binary_accuracy: 0.6920\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 3578s 894ms/step - loss: 0.3705 - binary_accuracy: 0.8510 - val_loss: 0.6881 - val_binary_accuracy: 0.6950\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 3445s 861ms/step - loss: 0.3064 - binary_accuracy: 0.8752 - val_loss: 0.7601 - val_binary_accuracy: 0.7080\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 3469s 867ms/step - loss: 0.2470 - binary_accuracy: 0.9003 - val_loss: 0.6880 - val_binary_accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 3504s 876ms/step - loss: 0.2123 - binary_accuracy: 0.9153 - val_loss: 0.8577 - val_binary_accuracy: 0.7290\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 3520s 880ms/step - loss: 0.1816 - binary_accuracy: 0.9370 - val_loss: 0.7795 - val_binary_accuracy: 0.7190\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 3463s 866ms/step - loss: 0.1779 - binary_accuracy: 0.9330 - val_loss: 0.8495 - val_binary_accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest validation loss\n",
    "\n",
    "weights_path = './models/rnn/ln_Keras_imdb_batchSize_1/best_model.h5'\n",
    "\n",
    "model_ln_keras.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1427/25000 [>.............................] - ETA: 2:03:49 - loss: 0.6841 - binary_accuracy: 0.6896ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000027790769B48>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 4167, in <genexpr>\n",
      "    ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))  File \"C:\\Users\\Rima\\.conda\\envs\\bln\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py\", line 237, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "25000/25000 [==============================] - 8517s 341ms/step - loss: 0.6875 - binary_accuracy: 0.6842\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_test_history = model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.writing pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_1/model_ln_keras_history_1.pickle', model_ln_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/ln_Keras_imdb_batchSize_1/model_ln_keras_test_history_1.pickle', model_ln_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_1/model_bn_keras_history_1.pickle', model_bn_keras_history.history)\n",
    "write_pickle_file('./logs/rnn/bn_Keras_imdb_batchSize_1/model_bn_keras_test_history_1.pickle', model_bn_keras_test_history)\n",
    "\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_1/model_bln_keras_history_1.pickle', model_bln_layer_history.history)\n",
    "write_pickle_file('./logs/rnn/bln_layer_imdb_batchSize_1/model_bln_keras_test_history_1.pickle', evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
