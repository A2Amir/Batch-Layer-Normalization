{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from utils import check_balance,visualize, creat_datasets, reset_graph, grid_serach, read_pick_file\n",
    "from layers import  bln_layer, dense_layer\n",
    "from callbacks import bln_callback , tensorboard_callback, create_callback_list, save_best_model_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=100\n",
    "minibatch = 20\n",
    "buffersize = 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "number_batches_train = 500 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 50 # number of batches to validate, each batch of size minibatch parameter\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = creat_datasets(x_train, y_train, x_test, y_test,\n",
    "                                                            number_valid_sampels = number_valid_sampels,\n",
    "                                                            random_seed = random_seed, minibatch = minibatch,\n",
    "                                                            buffersize = buffersize, num_classes = num_classes,\n",
    "                                                            reshape_to = 32*32*3, back_reshape = (-1,32,32,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tf.data.Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in valid_dataset.take(1) :\n",
    "    print(i.shape, j.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Batch Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (32,32,3), units1 = 512,\n",
    "                    num_classes = 10, random_seed = 100,\n",
    "                    batch_size = 60,\n",
    "                    b_mm = True, b_mv = True,\n",
    "                    f_mm = False, f_mv = False):\n",
    "    \n",
    "    # building the model\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(2, 2),strides=(2,2), padding=\"same\",\n",
    "                               kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (input_lyr)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn2', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn3', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    \n",
    "    # flattening the convolutions\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # fully-connected layer\n",
    "    x = dense_layer(units = units1, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn4', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x)\n",
    "    \n",
    "    output_lyr = dense_layer(units = num_classes, name = 'dense2', random_seed=random_seed)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape = (32,32,3), units1 = 512,\n",
    "                                 num_classes = num_classes, random_seed = random_seed,\n",
    "                                 batch_size = minibatch,\n",
    "                                 b_mm = True, b_mv = True,\n",
    "                                 f_mm = False, f_mv = False,\n",
    "                                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_model_bln_layer_TTFF_cifar10'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = create_callback_list(save_bm_cb, tb_cb, bln_layer_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs = epochs,\n",
    "                                                verbose = 1, callbacks = bln_layer_cb_list,\n",
    "                                                validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"./models/\" + folder_name + '/'+ str(number_batches_train) +\"_pretrained_weights_TTFF.h5\"\n",
    "model_bln_layer.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list, model_bln_layer_history\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True, \n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using  Batch Normalization implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape= (32,32,3), units1 = 512,\n",
    "                    num_classes = 10, random_seed = 100,\n",
    "                    batch_size = 60):\n",
    "    \n",
    "    # building the model\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(2, 2),strides=(2,2), padding=\"same\",\n",
    "                               kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (input_lyr)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum = 0.99, name = 'bn1')(x) \n",
    "    \n",
    "\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum = 0.99, name =  'bn2')(x) \n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum = 0.99, name = 'bn3')(x) \n",
    "    \n",
    "    \n",
    "    # flattening the convolutions\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # fully-connected layer\n",
    "    x = dense_layer(units = units1, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum = 0.99, name = 'bn4')(x)\n",
    "    output_lyr = dense_layer(units = num_classes, name = 'dense2', random_seed=random_seed)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape= (32,32,3), units1 = 512,\n",
    "                                num_classes = 10, random_seed = 100,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_bn_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_bn_Keras_cifar10'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=bn_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape= (32,32,3), units1 = 512,\n",
    "                    num_classes = 10, random_seed = 100,\n",
    "                    batch_size = 60):\n",
    "    \n",
    "    # building the model\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(2, 2),strides=(2,2), padding=\"same\",\n",
    "                               kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (input_lyr)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(2, 2), strides=(2,2), padding=\"same\", \n",
    "                                     kernel_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed)) (x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x) \n",
    "    \n",
    "    \n",
    "    # flattening the convolutions\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # fully-connected layer\n",
    "    x = dense_layer(units = units1, name = 'dense1', random_seed=random_seed)(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    output_lyr = dense_layer(units = num_classes, name = 'dense2', random_seed=random_seed)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (32,32,3), units1 = 512,\n",
    "                                num_classes = 10, random_seed = 100,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_ln_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_ln_Keras_cifar10'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,b = [], []\n",
    "for i in range(50):\n",
    "    b.append([i+1,1- (1/(i+1))+.0001])\n",
    "    f.append([i+1,1/(i+1)-.0001])\n",
    "\n",
    "f =  np.array(f)#shape\n",
    "b =  np.array(b)#shape\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(f[:,:1], f[:,1:], color='blue')\n",
    "ax.plot(b[:,:1], b[:,1:], color='red')\n",
    "\n",
    "plt.xlim([0, 50])\n",
    "plt.ylim([0, 1])\n",
    "plt.xticks(np.arange(0, 50, 1.0))\n",
    "plt.xticks(fontsize=14, rotation=90)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel('Batch Size',  fontsize=18)\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "plt.ylabel('Weight Amount',  fontsize=18)\n",
    "plt.legend([\"The amount of weight on last layer feature\",   \"The amount of weight on mini-batch \"],\n",
    "           bbox_to_anchor=(1.00, .95), loc='upper right', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
