{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from utils import creat_datasets, reset_graph, grid_serach,  creating_val_data\n",
    "from utils import creating_train_val_test_datasets\n",
    "from layers import  bln_layer, dense_layer\n",
    "from callbacks import bln_callback , tensorboard_callback, create_callback_list, save_best_model_callback\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=100\n",
    "minibatch = 20\n",
    "buffersize= 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs=5\n",
    "learning_rate = 0.001\n",
    "number_batches_train = 500 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 50 # number of batches to validate, each batch of size minibatch parameter\n",
    "num_classes = 1\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amira\\.conda\\envs\\Normalization\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\amira\\.conda\\envs\\Normalization\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = creating_val_data(x_train, y_train,\n",
    "                                                       number_valid_sampels = number_valid_sampels,\n",
    "                                                       random_seed=random_seed)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = creating_train_val_test_datasets(x_train, y_train,\n",
    "                                                                              x_test, y_test,\n",
    "                                                                              x_valid, y_valid, \n",
    "                                                                              minibatch = minibatch,\n",
    "                                                                              buffersize= buffersize,\n",
    "                                                                              random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((20, 80), (20,)), types: (tf.int32, tf.int64)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Custom Batch and Layer Normalization Layer(cBLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 10,\n",
    "                          b_mm = True, b_mv = True,\n",
    "                          f_mm = False, f_mv = False):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape= (80), max_features = max_features, \n",
    "                                        embed_size= 128, random_seed = random_seed,\n",
    "                                        lstm_unit = 128 , dense_units= num_classes,\n",
    "                                        batch_size = minibatch,\n",
    "                                        b_mm = True, b_mv = True,\n",
    "                                        f_mm = False, f_mv = False\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(20, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (20, 80, 128)             2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (20, 80, 128)             131584    \n",
      "_________________________________________________________________\n",
      "bn1 (bln_layer)              (20, 80, 128)             23938     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (20, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (20, 1)                   65        \n",
      "=================================================================\n",
      "Total params: 2,764,995\n",
      "Trainable params: 2,741,313\n",
      "Non-trainable params: 23,682\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_model_bln_layer_TTFF_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = create_callback_list(save_bm_cb, tb_cb, bln_layer_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                               loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                               metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 50 steps\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 347s 694ms/step - loss: 0.5269 - binary_accuracy: 0.7256 - val_loss: 0.4497 - val_binary_accuracy: 0.7940\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 352s 705ms/step - loss: 0.3320 - binary_accuracy: 0.8598 - val_loss: 0.4304 - val_binary_accuracy: 0.8160\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 354s 709ms/step - loss: 0.2248 - binary_accuracy: 0.9121 - val_loss: 0.4928 - val_binary_accuracy: 0.7880\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 352s 705ms/step - loss: 0.1545 - binary_accuracy: 0.9421 - val_loss: 0.5491 - val_binary_accuracy: 0.7830\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 365s 729ms/step - loss: 0.1071 - binary_accuracy: 0.9610 - val_loss: 0.6499 - val_binary_accuracy: 0.7830\n"
     ]
    }
   ],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs=epochs,\n",
    "                                                verbose=1, callbacks=bln_layer_cb_list,\n",
    "                                                validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 353s 282ms/step - loss: 0.6609 - binary_accuracy: 0.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6609152596205473, 0.77308]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bln_layer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"./models/\" + folder_name + '/'+ str(number_batches_train) +\"_pretrained_weights_TTFF.h5\"\n",
    "model_bln_layer.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list, model_bln_layer_history\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 372s 298ms/step - loss: 0.6583 - binary_accuracy: 0.7733\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 361s 289ms/step - loss: 0.6608 - binary_accuracy: 0.7730\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 368s 295ms/step - loss: 0.6583 - binary_accuracy: 0.7733\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 392s 313ms/step - loss: 0.6609 - binary_accuracy: 0.7731\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 448s 358ms/step - loss: 0.6826 - binary_accuracy: 0.7704\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 374s 299ms/step - loss: 0.6850 - binary_accuracy: 0.7705\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 360s 288ms/step - loss: 0.6826 - binary_accuracy: 0.7704\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 379s 304ms/step - loss: 0.6851 - binary_accuracy: 0.7705\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 371s 297ms/step - loss: 0.6905 - binary_accuracy: 0.7738\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 395s 316ms/step - loss: 0.6922 - binary_accuracy: 0.7732\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 423s 338ms/step - loss: 0.6905 - binary_accuracy: 0.7738\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 412s 329ms/step - loss: 0.6922 - binary_accuracy: 0.7731\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.6922299877732992, 0.77308]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 363s 290ms/step - loss: 0.7138 - binary_accuracy: 0.7712\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.6922299877732992, 0.77308], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.7137897999718785, 0.77124]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 396s 317ms/step - loss: 0.7156 - binary_accuracy: 0.7708\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.6922299877732992, 0.77308], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.7137897999718785, 0.77124], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.7155631319105625, 0.7708]}\n",
      "session is clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 424s 339ms/step - loss: 0.7138 - binary_accuracy: 0.7712\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.6922299877732992, 0.77308], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.7137897999718785, 0.77124], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.7155631319105625, 0.7708], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.7138029776692391, 0.77124]}\n",
      "session is clear\n",
      "1250/1250 [==============================] - 433s 347ms/step - loss: 0.7156 - binary_accuracy: 0.7708\n",
      "{'Bmm_True Bmv_True Fmm_True Fmv_True': [0.6582831666946412, 0.77328], 'Bmm_True Bmv_True Fmm_True Fmv_False': [0.6608336184233427, 0.77304], 'Bmm_True Bmv_True Fmm_False Fmv_True': [0.6583203714460134, 0.77328], 'Bmm_True Bmv_True Fmm_False Fmv_False': [0.6609152596205473, 0.77308], 'Bmm_True Bmv_False Fmm_True Fmv_True': [0.6825969819128513, 0.77044], 'Bmm_True Bmv_False Fmm_True Fmv_False': [0.6850093473628164, 0.77052], 'Bmm_True Bmv_False Fmm_False Fmv_True': [0.6826354142576456, 0.77044], 'Bmm_True Bmv_False Fmm_False Fmv_False': [0.6850927071258426, 0.77048], 'Bmm_False Bmv_True Fmm_True Fmv_True': [0.6904619035258889, 0.7738], 'Bmm_False Bmv_True Fmm_True Fmv_False': [0.6922044619083405, 0.7732], 'Bmm_False Bmv_True Fmm_False Fmv_True': [0.6904719975620508, 0.7738], 'Bmm_False Bmv_True Fmm_False Fmv_False': [0.6922299877732992, 0.77308], 'Bmm_False Bmv_False Fmm_True Fmv_True': [0.7137897999718785, 0.77124], 'Bmm_False Bmv_False Fmm_True Fmv_False': [0.7155631319105625, 0.7708], 'Bmm_False Bmv_False Fmm_False Fmv_True': [0.7138029776692391, 0.77124], 'Bmm_False Bmv_False Fmm_False Fmv_False': [0.7155948354110122, 0.7708]}\n",
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True,\n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path,\n",
    "                         loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics = tf.keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bmm_True Bmv_True Fmm_True Fmv_True', [0.6582831666946412, 0.77328]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_True', [0.6583203714460134, 0.77328]),\n",
       " ('Bmm_True Bmv_True Fmm_True Fmv_False', [0.6608336184233427, 0.77304]),\n",
       " ('Bmm_True Bmv_True Fmm_False Fmv_False', [0.6609152596205473, 0.77308]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_True', [0.6825969819128513, 0.77044]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_True', [0.6826354142576456, 0.77044]),\n",
       " ('Bmm_True Bmv_False Fmm_True Fmv_False', [0.6850093473628164, 0.77052]),\n",
       " ('Bmm_True Bmv_False Fmm_False Fmv_False', [0.6850927071258426, 0.77048]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_True', [0.6904619035258889, 0.7738]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_True', [0.6904719975620508, 0.7738]),\n",
       " ('Bmm_False Bmv_True Fmm_True Fmv_False', [0.6922044619083405, 0.7732]),\n",
       " ('Bmm_False Bmv_True Fmm_False Fmv_False', [0.6922299877732992, 0.77308]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_True', [0.7137897999718785, 0.77124]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_True', [0.7138029776692391, 0.77124]),\n",
       " ('Bmm_False Bmv_False Fmm_True Fmv_False', [0.7155631319105625, 0.7708]),\n",
       " ('Bmm_False Bmv_False Fmm_False Fmv_False', [0.7155948354110122, 0.7708])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using Batch Normalization layer implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn1') (x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(20, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (20, 80, 128)             2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (20, 80, 128)             131584    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (20, 80, 128)             512       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (20, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (20, 1)                   65        \n",
      "=================================================================\n",
      "Total params: 2,741,569\n",
      "Trainable params: 2,741,313\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 128, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_bn_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_bn_Keras_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 50 steps\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 425s 850ms/step - loss: 0.5278 - binary_accuracy: 0.7181 - val_loss: 0.5018 - val_binary_accuracy: 0.7320\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 411s 822ms/step - loss: 0.3049 - binary_accuracy: 0.8734 - val_loss: 0.5055 - val_binary_accuracy: 0.7650\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 386s 772ms/step - loss: 0.1744 - binary_accuracy: 0.9313 - val_loss: 0.5331 - val_binary_accuracy: 0.7790\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 409s 818ms/step - loss: 0.1064 - binary_accuracy: 0.9607 - val_loss: 0.6634 - val_binary_accuracy: 0.7970\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 410s 821ms/step - loss: 0.0708 - binary_accuracy: 0.9738 - val_loss: 0.7960 - val_binary_accuracy: 0.7850\n"
     ]
    }
   ],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs = epochs, verbose=1, \n",
    "                                             callbacks = bn_keras_cb_list,\n",
    "                                             validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 410s 328ms/step - loss: 0.8221 - binary_accuracy: 0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8220648340150714, 0.7768]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(20, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (20, 80, 128)             2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (20, 80, 128)             131584    \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (20, 80, 128)             256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (20, 64)                  49408     \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (20, 1)                   65        \n",
      "=================================================================\n",
      "Total params: 2,741,313\n",
      "Trainable params: 2,741,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 128, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_ln_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_ln_Keras_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 500 steps, validate for 50 steps\n",
      "Epoch 1/5\n",
      "500/500 [==============================] - 400s 801ms/step - loss: 0.5214 - binary_accuracy: 0.7248 - val_loss: 0.4643 - val_binary_accuracy: 0.7680\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 367s 733ms/step - loss: 0.2982 - binary_accuracy: 0.8783 - val_loss: 0.4439 - val_binary_accuracy: 0.7910\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 367s 734ms/step - loss: 0.1770 - binary_accuracy: 0.9322 - val_loss: 0.5416 - val_binary_accuracy: 0.8010\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 385s 769ms/step - loss: 0.1138 - binary_accuracy: 0.9575 - val_loss: 0.6843 - val_binary_accuracy: 0.7440\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 373s 746ms/step - loss: 0.0707 - binary_accuracy: 0.9756 - val_loss: 0.8464 - val_binary_accuracy: 0.7590\n"
     ]
    }
   ],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 391s 313ms/step - loss: 0.8189 - binary_accuracy: 0.7628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8189312511198222, 0.76284]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is clear\n"
     ]
    }
   ],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
