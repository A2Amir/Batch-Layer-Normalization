{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from utils import creat_datasets, reset_graph, grid_serach,  creating_val_data\n",
    "from utils import creating_train_val_test_datasets\n",
    "from layers import  bln_layer, dense_layer\n",
    "from callbacks import bln_callback , tensorboard_callback, create_callback_list, save_best_model_callback\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=100\n",
    "minibatch = 20\n",
    "buffersize= 60000\n",
    "number_valid_sampels = 5000 # number of validation data\n",
    "epochs=5\n",
    "learning_rate = 0.001\n",
    "number_batches_train = 500 # number of batches to train, each batch of size minibatch parameter\n",
    "number_batches_valid = 50 # number of batches to validate, each batch of size minibatch parameter\n",
    "num_classes = 1\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = creating_val_data(x_train, y_train,\n",
    "                                                       number_valid_sampels = number_valid_sampels,\n",
    "                                                       random_seed=random_seed)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = creating_train_val_test_datasets(x_train, y_train,\n",
    "                                                                              x_test, y_test,\n",
    "                                                                              x_valid, y_valid, \n",
    "                                                                              minibatch = minibatch,\n",
    "                                                                              buffersize= buffersize,\n",
    "                                                                              random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using Custom Batch and Layer Normalization Layer(cBLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLNLayer_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 10,\n",
    "                          b_mm = True, b_mv = True,\n",
    "                          f_mm = False, f_mv = False):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x = bln_layer(stateful = True, batchsize= batch_size, name = 'bn1', \n",
    "                  batch_moving_mean = b_mm, batch_moving_var = b_mv,\n",
    "                  feature_moving_mean = f_mm, feature_moving_var = f_mv)(x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer = BLNLayer_model(inputshape= (80), max_features = max_features, \n",
    "                                        embed_size= 128, random_seed = random_seed,\n",
    "                                        lstm_unit = 128 , dense_units= num_classes,\n",
    "                                        batch_size = minibatch,\n",
    "                                        b_mm = True, b_mv = True,\n",
    "                                        f_mm = False, f_mv = False\n",
    "                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_model_bln_layer_TTFF_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "# Callback for resetting moving mean and variances at the end of each epoch\n",
    "bln_layer_cb = bln_callback()\n",
    "\n",
    "bln_layer_cb_list = create_callback_list(save_bm_cb, tb_cb, bln_layer_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                               loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                               metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_bln_layer_history =  model_bln_layer.fit(train_dataset.take(number_batches_train), epochs=epochs,\n",
    "                                                verbose=1, callbacks=bln_layer_cb_list,\n",
    "                                                validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bln_layer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"./models/\" + folder_name + '/'+ str(number_batches_train) +\"_pretrained_weights_TTFF.h5\"\n",
    "model_bln_layer.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bln_layer, save_bm_cb, tb_cb, bln_layer_cb, bln_layer_cb_list, model_bln_layer_history\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_path = \"./logs/\" + folder_name + '/'+ str(number_batches_train) +\"_sorted_evaluation.pkl\"\n",
    "evaluation = grid_serach(BLNLayer_model, test_dataset,\n",
    "                         batch_size = minibatch, sort=True,\n",
    "                         save_eval_path = save_eval_path,\n",
    "                         weights_path = weights_path,\n",
    "                         loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                         metrics = tf.keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Using Batch Normalization layer implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_keras_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.BatchNormalization(momentum = 0.99,  name = 'bn1') (x) \n",
    "    \n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras = bn_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 128, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_bn_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_bn_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_bn_Keras_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "\n",
    "bn_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras_history =  model_bn_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs = epochs, verbose=1, \n",
    "                                             callbacks = bn_keras_cb_list,\n",
    "                                             validation_data = valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bn_keras, save_bm_cb, tb_cb, bn_keras_cb_list \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Using  Layer normalization  implemented in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_keras_model(inputshape= (80), max_features = 20000, embed_size=128, random_seed = 100,\n",
    "                          lstm_unit = 128 , dense_units= 1,\n",
    "                          batch_size = 60):\n",
    "    \n",
    "\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=batch_size, name = 'input')\n",
    "    \n",
    "    x = Embedding(max_features, embed_size,\n",
    "                  embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=random_seed))(input_lyr)\n",
    "    \n",
    "    x = LSTM(lstm_unit, dropout=0.2, recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=True)(x)\n",
    "    \n",
    "    x =  tf.keras.layers.LayerNormalization()(x) \n",
    "\n",
    "    x = LSTM(lstm_unit//2, dropout=0.2,  recurrent_dropout=0.2,\n",
    "            recurrent_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed),\n",
    "            kernel_initializer = tf.keras.initializers.GlorotUniform(seed=random_seed) ,\n",
    "            return_sequences=False)(x)\n",
    "\n",
    "    \n",
    "    output_lyr = dense_layer(units = dense_units, name = 'dense1', random_seed=random_seed)(x)    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras = ln_keras_model(inputshape= (80), max_features = max_features, \n",
    "                                embed_size= 128, random_seed = random_seed,\n",
    "                                lstm_unit = 128 , dense_units= num_classes,\n",
    "                                batch_size = minibatch)\n",
    "model_ln_keras.summary()\n",
    "\n",
    "#### Compiling \n",
    "model_ln_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving best model and tensorboard\n",
    "folder_name = str(number_batches_train) + '_ln_Keras_imdb'\n",
    "save_bm_cb = save_best_model_callback(folder_name)\n",
    "tb_cb = tensorboard_callback(folder_name)\n",
    "\n",
    "ln_keras_cb_list = [save_bm_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras_history =  model_ln_keras.fit(train_dataset.take(number_batches_train),\n",
    "                                             epochs=epochs, verbose=1, \n",
    "                                             callbacks=ln_keras_cb_list,\n",
    "                                             validation_data=valid_dataset.take(number_batches_valid),\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln_keras.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_ln_keras,save_bm_cb, ln_keras_cb, ln_keras_cb_list\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
